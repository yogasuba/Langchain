Document Q&A System
A flexible document question-answering system that can run both with OpenAI APIs and completely locally without any external services.

Features
ğŸ“„ Support for PDF and Text documents

ğŸ” Semantic search using vector embeddings

ğŸ’¬ Question-answering with relevant context

ğŸŒ Optional OpenAI integration for advanced responses

ğŸ’¯ 100% local mode available (no internet required)

ğŸš€ Fast and efficient document processing

Installation
Prerequisites
Python 3.8+

pip package manager

Install Dependencies
bash
# Basic installation
pip install langchain langchain-community langchain-text-splitters chromadb

# For local embeddings (FREE mode)
pip install sentence-transformers torch

# For OpenAI integration (optional)
pip install langchain-openai openai

# For PDF support
pip install pypdf
Usage
Method 1: Local Mode (FREE - No API Keys Required)
Run the system completely locally without any external services:

bash
python poc.py

Features:

Uses local embedding models (all-MiniLM-L6-v2)

No internet connection required after first run

No costs or API limits

Document search and retrieval

Method 2: OpenAI Mode (With API Key)
Run with OpenAI for enhanced question-answering:

bash
# Set your OpenAI API key
set OPENAI_API_KEY=your_api_key_here

# Run the script
python poc.py
Features:

Advanced GPT-powered responses

Better contextual understanding

More natural language generation

File Structure
text
document-qa-system/
â”‚
â”œâ”€â”€ poc.py                 # Main script with OpenAI integration
â”œâ”€â”€ poc_local.py           # Completely local version
â”œâ”€â”€ sample.txt             # Example document
â”œâ”€â”€ chroma_db/            # Vector database storage
â””â”€â”€ README.md             # This file
How It Works
Document Loading: Reads your PDF or text files

Text Splitting: Breaks documents into manageable chunks

Embedding Generation: Converts text to numerical vectors

Local: Uses HuggingFace models

OpenAI: Uses OpenAI's embedding API

Vector Storage: Stores embeddings in ChromaDB

Similarity Search: Finds relevant document sections for queries

Answer Generation: Provides answers based on retrieved context

Supported Document Formats
PDF (.pdf) - Research papers, manuals, books

Text (.txt) - Notes, articles, documentation

Example Questions to Ask
For Technical Documents:
text
What is the main concept discussed?
Explain [technical term] in simple terms
What are the key findings?
How does [process] work?
For Research Papers:
text
What is the research methodology?
What are the conclusions?
What problem does this solve?
What data was used?
For Manuals/Guides:
text
How do I perform [specific task]?
What are the system requirements?
Troubleshooting steps for [issue]
Configuration
Local Mode Settings
Embedding Model: sentence-transformers/all-MiniLM-L6-v2

Chunk Size: 1000 characters

Chunk Overlap: 200 characters

Similarity Search: Top 3 results

OpenAI Mode Settings
Model: GPT-3.5-turbo or GPT-4

Temperature: 0 (deterministic responses)

Max Tokens: Based on context

Troubleshooting
Common Issues
Module Not Found Errors

bash
pip install --upgrade langchain-community
PDF Reading Issues

bash
pip install --upgrade pypdf
Local Model Download Problems

Check internet connection for first-time download

Ensure sufficient disk space (~400MB for embeddings model)

OpenAI API Errors

Verify API key is set correctly

Check billing and usage limits

Ensure organization settings allow API access

Performance Tips
For large documents, increase chunk size to 1500-2000 characters

Use SSD storage for faster vector database operations

Close other memory-intensive applications when processing large files

For better local performance, consider using GPU with CUDA support

Privacy & Security
Local Mode
âœ… All processing happens on your machine

âœ… No data sent to external services

âœ… Complete privacy and control

OpenAI Mode
âš ï¸ Document content sent to OpenAI servers

ğŸ”’ Follows OpenAI's data usage policies

ğŸ“Š Check OpenAI's privacy policy for details
